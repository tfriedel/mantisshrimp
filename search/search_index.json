{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Mantisshrimp: Agnostic Object Detection Framework We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. We Need Your Help If you find this work useful, please let other people know by starring it, and sharing it on GitHub . Thank you Quick Example: How to train the PETS Dataset Source Code The Problem We Are Solving Object dectection datasets come in different sizes and most impotantly have different annotations formats ranging from the stanndard formarts such COCO and VOC to more self-tailored formats When new object detection models are released with some source code, the latter is very often written in non-portable way: The source code is difficult to use for other datasets because of some hard-coded parts coupled with self developed tweaks Both researchers and DL coders have to deploy a lot of effort to use many SOTA models for their own use-cases and/or to craft an enhanced model based on those already published Our Solution Mantisshrimp library provides some elegant solutions in those 2 fundamental components: 1- A Unified Data API: Out of the box, we offer several annotation parsers that translates different annotation formats into a very flexibe parser: A. By default, we offer differents standard format parsers such as COCO and ROC, B. We host a community curated parsers where community contributors publish their own parsers to be shared, and therefore save time and energy in creating similar parsers over and over, C. We provide some intuitive tutorials that walk you through the steps of creating your own parser. Please, consider sharing it with the whole community. 2- A Universal Adapter to different DL Libraries: A. Mantisshrimp provides a universal adapter that allows you to hook up your dataset to the DL library of your choice (fastai, Pytorch Lightning and Pytorch), and train your model using a familiar API, B. Our library allows you to choose one of the public implementations of a given model, plug it in mantisshrimp model adapter, and seamlessly train your model, C. As a bonus, our library even allows to experiment with another DL library. Our tutorials have several examples showing you how to train a given model using both fastai and Pytorch Lightning libraries side by side.","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#_2","text":"Mantisshrimp: Agnostic Object Detection Framework We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. We Need Your Help If you find this work useful, please let other people know by starring it, and sharing it on GitHub . Thank you","title":""},{"location":"#quick-example-how-to-train-the-pets-dataset","text":"Source Code","title":"Quick Example: How to train the PETS Dataset"},{"location":"#the-problem-we-are-solving","text":"Object dectection datasets come in different sizes and most impotantly have different annotations formats ranging from the stanndard formarts such COCO and VOC to more self-tailored formats When new object detection models are released with some source code, the latter is very often written in non-portable way: The source code is difficult to use for other datasets because of some hard-coded parts coupled with self developed tweaks Both researchers and DL coders have to deploy a lot of effort to use many SOTA models for their own use-cases and/or to craft an enhanced model based on those already published","title":"The Problem We Are Solving"},{"location":"#our-solution","text":"Mantisshrimp library provides some elegant solutions in those 2 fundamental components: 1- A Unified Data API: Out of the box, we offer several annotation parsers that translates different annotation formats into a very flexibe parser: A. By default, we offer differents standard format parsers such as COCO and ROC, B. We host a community curated parsers where community contributors publish their own parsers to be shared, and therefore save time and energy in creating similar parsers over and over, C. We provide some intuitive tutorials that walk you through the steps of creating your own parser. Please, consider sharing it with the whole community. 2- A Universal Adapter to different DL Libraries: A. Mantisshrimp provides a universal adapter that allows you to hook up your dataset to the DL library of your choice (fastai, Pytorch Lightning and Pytorch), and train your model using a familiar API, B. Our library allows you to choose one of the public implementations of a given model, plug it in mantisshrimp model adapter, and seamlessly train your model, C. As a bonus, our library even allows to experiment with another DL library. Our tutorials have several examples showing you how to train a given model using both fastai and Pytorch Lightning libraries side by side.","title":"Our Solution"},{"location":"about/","text":"Hall of Fame This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"About"},{"location":"about/#hall-of-fame","text":"This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"Hall of Fame"},{"location":"advanced_guide/","text":"Building a Custom Parser and a Custom Model Goal In this book, we will show how to: Create a custom Parser: we are using the Wheat example for the Kaggle Global Wheat Competition Train the model using Fastai library Train the model using Pytorch-Lightning library Use a custom backbone Save a trained model Make predictions using our Inference API Install Simple have Pytorch and get this from GitHub (release on PyPi to be soon) pip install -r requirements.txt pip install git+git://github.com/lgvaz/mantisshrimp.git ! pip install - q 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' ! pip install - q fastai2 ! pip install - q pytorch_lightning ! pip - q install git + git : // github . com / lgvaz / mantisshrimp . git Getting started Imports Mantisshrimp is built in such a way that is safe to use wildcard imports, .e.g. from mantisshrimp import *. from mantisshrimp.imports import * will import commonly used packages like np and plt. from mantisshrimp import * will import all mantis modules needed for development. from mantisshrimp.imports import * from mantisshrimp import * import pandas as pd import albumentations as A Loading the data The first step is to understand the data. In this task we were given a .csv file with annotations, let's take a look at that. Note: Replace source with your own path for the dataset directory. Parser To process the data into format feedable to the models we need to create a parser. path = Path ( '../input/global-wheat-detection/' ) df = pd . read_csv ( path / \"train.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 0 b6ab77fd7 1024 1024 [834.0, 222.0, 56.0, 36.0] usask_1 1 b6ab77fd7 1024 1024 [226.0, 548.0, 130.0, 58.0] usask_1 2 b6ab77fd7 1024 1024 [377.0, 504.0, 74.0, 160.0] usask_1 3 b6ab77fd7 1024 1024 [834.0, 95.0, 109.0, 107.0] usask_1 4 b6ab77fd7 1024 1024 [26.0, 144.0, 124.0, 117.0] usask_1 At first glance, we can make the following assumptions: Multiple rows with the same object_id, width, height A different bbox for each row source doesn't seem relevant right now Once we know what our data provides we can create our custom Parser. When creating a Parser we inherit from smaller building blocks that provides the functionallity we want: DefaultImageInfoParser: Will parse standard fields for image information, e.g. filepath, height, width FasterRCNNParser: Since we only need to predict bboxes we will use a FasterRCNN model, this will parse all the requirements for using such a model. We can also specify exactly what fields we would like to parse, in fact, the parsers we are currently using are just helper classes that groups a collection of individual parsers. We are going to see how to use individual parsers in a future tutorial. Defining the init is completely up to you, normally we have to pass our data (the df in our case) and the folder where our images are contained (source in our case). We then override iter , telling our parser how to iterate over our data. In our case we call df.itertuples to iterate over all df rows. len is not obligatory but will help visualizing the progress when parsing. And finally we override all the other methods, they all receive a single argument o, which is the object returned by iter . Now we just need to decide how to split our data and Parser.parse! class WheatParser ( DefaultImageInfoParser , FasterRCNNParser ): def __init__ ( self , df , source ): self . df = df self . source = source self . imageid_map = IDMap () def __iter__ ( self ): yield from self . df . itertuples () def __len__ ( self ): return len ( self . df ) def imageid ( self , o ) -> int : return self . imageid_map [ o . image_id ] def filepath ( self , o ) -> Union [ str , Path ]: return self . source / f \" { o . image_id } .jpg\" def height ( self , o ) -> int : return o . height def width ( self , o ) -> int : return o . width def labels ( self , o ) -> List [ int ]: return [ 1 ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))] Mantisshrimp eliminates boilerplate for you :-) It creates a RandomSpitter which divides data into trainand test. Then we create the parser and simply parse the data data_splitter = RandomSplitter ([ . 8 , . 2 ]) parser = WheatParser ( df , source / \"train\" ) train_rs , valid_rs = parser . parse ( data_splitter ) HBox(children=(FloatProgress(value=0.0, max=147793.0), HTML(value=''))) Let's take a look at one record. show_record ( train_rs [ 0 ], label = False ) Transforms Mantisshrimp is agnostic to the transform library you want to use. We provide default support for albumentations but if you want to use another library you just need to inherit and override all abstract methods of Transform. For simplicity, let's use a single transform on the train data and no transforms on the validation data. train_tfm = AlbuTransform ([ A . Flip ()]) Datasets This is equivalent to PyTorch datasets that we use always. For creating a Dataset we just need need to pass the parsed records from the previous step and optionally a transform. train_ds = Dataset ( train_rs , train_tfm ) valid_ds = Dataset ( valid_rs ) Model It uses the torchvision implementation of Faster RCNN which most people are using here. Best part, we can try Faster RCNN with multiple Backbones, and its hardly any line of code. It allows flexible model implementation coz we don't want to limit the library from mantisshrimp.models.rcnn.faster_rcnn import * from mantisshrimp.models.rcnn import * # Using the FasterRCNN Basic Model with resnet50 fpn backbone model = faster_rcnn . model ( num_classes = 2 ) Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value=''))) Backbones Backbones define which CNN is being used as feature extractor or base of the model. Mantisshrimp supports all torchvision CNN models as well models from PyTorch Image models by ross wightman. from mantisshrimp import backbones It supports many resnet models with fpn on top of it. You can use pretrained imagenet Backbones as well. Is supports backbones \"resnet18\", \"resnet34\",\"resnet50\", \"resnet101\", \"resnet152\", \"resnext50_32x4d\", \"resnext101_32x8d\", \"wide_resnet50_2\", \"wide_resnet101_2\", as resnets with fpn backbones. resnet_101_backbone = backbones . resnet_fpn . resnet101 ( pretrained = True ) Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value=''))) resnet_152_backbone = backbones . resnet_fpn . resnet152 ( pretrained = True ) Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value=''))) It supports backbones \"resnet18\", \"resnet34\", \"resnet50\",\"resnet101\", \"resnet152\", \"resnext101_32x8d\", \"mobilenet\", \"vgg11\", \"vgg13\", \"vgg16\", \"vgg19\", without fpn networks vgg11_backbone = backbones . vgg . vgg11 ( pretrained = True ) Downloading: \"https://download.pytorch.org/models/vgg11-bbd30ac9.pth\" to /root/.cache/torch/checkpoints/vgg11-bbd30ac9.pth HBox(children=(FloatProgress(value=0.0, max=531456000.0), HTML(value=''))) Mantisshrimp Engines Now engine enters the picture. We provide out of box support to train with Fastai and PyTorch Lightning. You can use any one, at end of the day both remain PyTorch models. Training Using Fastai We provide support for COCO Metrics too, very useful while training # metrics = [] # metrics += [COCOMetric(valid_rs, bbox=True, mask=False, keypoint=False)] DataLoader Another feature is that all mantis models have a dataloader method that returns a customized DataLoader for each model. train_dl = faster_rcnn . dataloaders . train_dataloader ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . dataloaders . valid_dataloader ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) # This creates the default model with resnet50 fpn backbone # model = faster_rcnn.model(num_classes=2) # To create model with backbones model = faster_rcnn . model ( num_classes = 2 , backbone = resnet_152_backbone ) learner = faster_rcnn . fastai . learner ([ train_dl , valid_dl ], model ) learner . fine_tune ( 2 , lr = 1e-4 ) /opt/conda/lib/python3.7/site-packages/fastai2/callback/core.py:29: UserWarning: You are setting an attribute (loss) that also exists in the learner. Please be advised that you're not setting it in the learner but in the callback. Use `self.learn.loss` if you would like to change it in the learner. warn(f\"You are setting an attribute ({name}) that also exists in the learner. Please be advised that you're not setting it in the learner but in the callback. Use `self.learn.{name}` if you would like to change it in the learner.\") epoch train_loss valid_loss time 0 1.262132 1.240003 10:35 /opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated: nonzero(Tensor input, *, Tensor out) Consider using one of the following signatures instead: nonzero(Tensor input, *, bool as_tuple) epoch train_loss valid_loss time 0 0.682398 0.660053 18:55 1 0.576909 0.555283 18:53 Training using PyTorch Lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): opt = SGD ( self . parameters (), 2e-4 , momentum = 0.9 ) return opt model = faster_rcnn . model ( num_classes = 2 , backbone = resnet_152_backbone ) light_model = LightModel ( model = model ) #metrics=metrics) from pytorch_lightning import Trainer trainer = Trainer ( max_epochs = 2 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) GPU available: True, used: True TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------- 0 | model | FasterRCNN | 75 M HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout\u2026 /opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated: nonzero(Tensor input, *, Tensor out) Consider using one of the following signatures instead: nonzero(Tensor input, *, bool as_tuple) HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max\u2026 HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m\u2026 HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m\u2026 1 Saving the Model # Save the model as same as you would do for a Pytorch model # You can also use lightning features to even automate this. torch . save ( light_model . state_dict (), \"mantiss_faster_rcnn.pt\" ) Making Predictions detection_threshold = 0.45 device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) def format_prediction_string ( boxes , scores ): pred_strings = [] for s , b in zip ( scores , boxes . astype ( int )): pred_strings . append ( f ' { s : .4f } { b [ 0 ] } { b [ 1 ] } { b [ 2 ] - b [ 0 ] } { b [ 3 ] - b [ 1 ] } ' ) return \" \" . join ( pred_strings ) # Just using the Fastai model for now. You can replace this with Light model as well. model . eval () model . to ( device ) detection_threshold = 1e-8 results = [] device = 'cuda' for images in os . listdir ( \"../input/global-wheat-detection/test/\" ): image_path = os . path . join ( \"../input/global-wheat-detection/test/\" , images ) image = cv2 . imread ( image_path ) image = np . transpose ( image , ( 2 , 0 , 1 )) image = image / 255. image = torch . tensor ( image , dtype = torch . float ) image = torch . unsqueeze ( image , 0 ) image = image . to ( device ) model # print(image.shape) with torch . no_grad (): outputs = model ( image ) # print(outputs) boxes = outputs [ 0 ][ 'boxes' ] . data . cpu () . numpy () scores = outputs [ 0 ][ 'scores' ] . data . cpu () . numpy () boxes = boxes [ scores >= detection_threshold ] . astype ( np . int32 ) scores = scores [ scores >= detection_threshold ] image_id = images [: - 3 ] result = { 'image_id' : image_id , 'PredictionString' : format_prediction_string ( boxes , scores ) } results . append ( result ) # break test_df = pd . DataFrame ( results , columns = [ 'image_id' , 'PredictionString' ]) test_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id PredictionString 0 796707dd7. 0.9835 892 326 108 95 0.9809 702 824 111 105 0.9749 499 780 102 102 0.9732 0 445 68 90 0.9714 349 0 83 61 0.9678 939 66 82 111 0.9501 134 545 138 110 0.9491 675 709 82 100 0.9306 611 34 100 83 0.9283 103 792 100 88 0.9190 44 89 170 106 0.9069 249 474 93 106 0.9055 674 27 126 129 0.8951 680 448 101 154 0.8831 374 621 99 140 0.8831 461 269 122 103 0.8787 220 835 122 82 0.8770 426 185 83 89 0.8714 246 322 82 161 0.8611 301 275 88 125 0.8575 663 559 122 151 0.8444 801 530 135 154 0.8337 235 0 113 47 0.8137 388 619 82 79 0.7730 214 675 91 82 0.6770 194 471 81 77 0.6689 592 490 79 77 0.6685 59 5... 1 cc3532ff6. 0.9866 369 0 97 106 0.9841 911 125 106 92 0.9761 606 423 94 106 0.9742 489 568 100 149 0.9719 258 631 107 179 0.9694 689 471 151 93 0.9693 470 395 124 162 0.9675 716 294 118 95 0.9632 552 826 96 100 0.9520 33 342 98 73 0.9487 306 286 92 87 0.9417 96 609 86 151 0.9375 401 206 87 88 0.9368 755 829 182 176 0.9332 0 418 164 99 0.9327 779 696 111 108 0.9302 72 816 138 162 0.9267 952 1 72 86 0.9106 564 299 104 112 0.8850 769 2 101 128 0.8816 14 657 104 71 0.7850 6 475 73 78 0.7729 26 644 173 87 0.7387 10 363 133 179 0.7025 583 889 103 112 0.6457 0 769 43 71 0.6198 291 839 101 66 0.5801 10 404 84... 2 51f1be19e. 0.9569 18 0 99 68 0.9546 274 464 155 134 0.9402 765 886 140 100 0.9393 636 793 111 76 0.9368 194 917 107 101 0.9321 688 914 98 84 0.9262 835 338 136 119 0.9229 600 95 153 166 0.9224 811 84 108 75 0.9153 815 757 91 86 0.9068 325 141 142 138 0.8867 902 693 105 82 0.8552 669 595 104 84 0.8491 240 117 88 119 0.8478 492 487 111 85 0.8331 860 271 104 204 0.8250 69 692 91 84 0.8121 563 588 113 128 0.7668 485 477 202 92 0.7495 0 379 49 106 0.6596 527 970 91 54 0.6227 534 462 178 89 0.5940 46 520 70 80 0.5751 659 428 83 72 0.5720 775 24 102 65 0.5616 74 684 82 165 0.5557 850 276 117 104 0.4984 120 ... 3 51b3e36ab. 0.9824 361 151 110 106 0.9809 471 577 88 85 0.9798 499 190 102 81 0.9786 0 901 86 121 0.9712 231 635 98 180 0.9694 420 925 88 99 0.9683 12 823 90 98 0.9578 440 323 150 97 0.9573 856 711 82 81 0.9540 377 424 108 82 0.9531 874 185 131 80 0.9519 462 4 87 164 0.9514 873 287 151 136 0.9464 329 466 91 152 0.9425 600 762 171 115 0.9412 113 848 143 90 0.9127 755 817 208 122 0.9046 534 29 271 129 0.8943 503 394 120 93 0.8894 11 435 82 213 0.8806 822 455 199 155 0.8713 609 374 174 95 0.8594 6 0 79 172 0.7965 7 580 87 188 0.7126 707 670 157 80 0.6970 1 412 103 365 0.5088 448 320 173 162 0.4831 553 24... 4 f5a1f0358. 0.9833 543 401 84 96 0.9782 683 197 122 101 0.9779 411 168 68 74 0.9767 63 457 136 119 0.9727 258 657 95 82 0.9725 659 104 88 92 0.9723 596 723 109 93 0.9701 224 317 124 99 0.9700 815 404 100 99 0.9684 1 831 65 131 0.9676 297 455 155 103 0.9672 881 628 97 168 0.9626 153 243 69 83 0.9616 149 758 163 123 0.9600 126 612 72 79 0.9447 224 558 80 92 0.9405 299 568 76 80 0.9387 2 5 64 66 0.9322 541 277 108 113 0.9281 692 564 85 135 0.9183 527 0 110 105 0.9134 405 677 84 123 0.9094 949 440 75 192 0.8918 87 818 67 75 0.8481 470 796 135 87 0.8111 459 314 85 156 0.6798 783 578 86 97 0.5494 459 575 56...","title":"Advanced Guide"},{"location":"advanced_guide/#building-a-custom-parser-and-a-custom-model","text":"","title":"Building a Custom Parser and a Custom Model"},{"location":"advanced_guide/#goal","text":"In this book, we will show how to: Create a custom Parser: we are using the Wheat example for the Kaggle Global Wheat Competition Train the model using Fastai library Train the model using Pytorch-Lightning library Use a custom backbone Save a trained model Make predictions using our Inference API","title":"Goal"},{"location":"advanced_guide/#install","text":"Simple have Pytorch and get this from GitHub (release on PyPi to be soon) pip install -r requirements.txt pip install git+git://github.com/lgvaz/mantisshrimp.git ! pip install - q 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' ! pip install - q fastai2 ! pip install - q pytorch_lightning ! pip - q install git + git : // github . com / lgvaz / mantisshrimp . git","title":"Install"},{"location":"advanced_guide/#getting-started","text":"","title":"Getting started"},{"location":"advanced_guide/#imports","text":"Mantisshrimp is built in such a way that is safe to use wildcard imports, .e.g. from mantisshrimp import *. from mantisshrimp.imports import * will import commonly used packages like np and plt. from mantisshrimp import * will import all mantis modules needed for development. from mantisshrimp.imports import * from mantisshrimp import * import pandas as pd import albumentations as A","title":"Imports"},{"location":"advanced_guide/#loading-the-data","text":"The first step is to understand the data. In this task we were given a .csv file with annotations, let's take a look at that. Note: Replace source with your own path for the dataset directory.","title":"Loading the data"},{"location":"advanced_guide/#parser","text":"To process the data into format feedable to the models we need to create a parser. path = Path ( '../input/global-wheat-detection/' ) df = pd . read_csv ( path / \"train.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 0 b6ab77fd7 1024 1024 [834.0, 222.0, 56.0, 36.0] usask_1 1 b6ab77fd7 1024 1024 [226.0, 548.0, 130.0, 58.0] usask_1 2 b6ab77fd7 1024 1024 [377.0, 504.0, 74.0, 160.0] usask_1 3 b6ab77fd7 1024 1024 [834.0, 95.0, 109.0, 107.0] usask_1 4 b6ab77fd7 1024 1024 [26.0, 144.0, 124.0, 117.0] usask_1 At first glance, we can make the following assumptions: Multiple rows with the same object_id, width, height A different bbox for each row source doesn't seem relevant right now Once we know what our data provides we can create our custom Parser. When creating a Parser we inherit from smaller building blocks that provides the functionallity we want: DefaultImageInfoParser: Will parse standard fields for image information, e.g. filepath, height, width FasterRCNNParser: Since we only need to predict bboxes we will use a FasterRCNN model, this will parse all the requirements for using such a model. We can also specify exactly what fields we would like to parse, in fact, the parsers we are currently using are just helper classes that groups a collection of individual parsers. We are going to see how to use individual parsers in a future tutorial. Defining the init is completely up to you, normally we have to pass our data (the df in our case) and the folder where our images are contained (source in our case). We then override iter , telling our parser how to iterate over our data. In our case we call df.itertuples to iterate over all df rows. len is not obligatory but will help visualizing the progress when parsing. And finally we override all the other methods, they all receive a single argument o, which is the object returned by iter . Now we just need to decide how to split our data and Parser.parse! class WheatParser ( DefaultImageInfoParser , FasterRCNNParser ): def __init__ ( self , df , source ): self . df = df self . source = source self . imageid_map = IDMap () def __iter__ ( self ): yield from self . df . itertuples () def __len__ ( self ): return len ( self . df ) def imageid ( self , o ) -> int : return self . imageid_map [ o . image_id ] def filepath ( self , o ) -> Union [ str , Path ]: return self . source / f \" { o . image_id } .jpg\" def height ( self , o ) -> int : return o . height def width ( self , o ) -> int : return o . width def labels ( self , o ) -> List [ int ]: return [ 1 ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))]","title":"Parser"},{"location":"advanced_guide/#mantisshrimp-eliminates-boilerplate-for-you-","text":"It creates a RandomSpitter which divides data into trainand test. Then we create the parser and simply parse the data data_splitter = RandomSplitter ([ . 8 , . 2 ]) parser = WheatParser ( df , source / \"train\" ) train_rs , valid_rs = parser . parse ( data_splitter ) HBox(children=(FloatProgress(value=0.0, max=147793.0), HTML(value=''))) Let's take a look at one record. show_record ( train_rs [ 0 ], label = False )","title":"Mantisshrimp eliminates boilerplate for you :-)"},{"location":"advanced_guide/#transforms","text":"Mantisshrimp is agnostic to the transform library you want to use. We provide default support for albumentations but if you want to use another library you just need to inherit and override all abstract methods of Transform. For simplicity, let's use a single transform on the train data and no transforms on the validation data. train_tfm = AlbuTransform ([ A . Flip ()])","title":"Transforms"},{"location":"advanced_guide/#datasets","text":"This is equivalent to PyTorch datasets that we use always. For creating a Dataset we just need need to pass the parsed records from the previous step and optionally a transform. train_ds = Dataset ( train_rs , train_tfm ) valid_ds = Dataset ( valid_rs )","title":"Datasets"},{"location":"advanced_guide/#model","text":"It uses the torchvision implementation of Faster RCNN which most people are using here. Best part, we can try Faster RCNN with multiple Backbones, and its hardly any line of code. It allows flexible model implementation coz we don't want to limit the library from mantisshrimp.models.rcnn.faster_rcnn import * from mantisshrimp.models.rcnn import * # Using the FasterRCNN Basic Model with resnet50 fpn backbone model = faster_rcnn . model ( num_classes = 2 ) Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))","title":"Model"},{"location":"advanced_guide/#backbones","text":"Backbones define which CNN is being used as feature extractor or base of the model. Mantisshrimp supports all torchvision CNN models as well models from PyTorch Image models by ross wightman. from mantisshrimp import backbones It supports many resnet models with fpn on top of it. You can use pretrained imagenet Backbones as well. Is supports backbones \"resnet18\", \"resnet34\",\"resnet50\", \"resnet101\", \"resnet152\", \"resnext50_32x4d\", \"resnext101_32x8d\", \"wide_resnet50_2\", \"wide_resnet101_2\", as resnets with fpn backbones. resnet_101_backbone = backbones . resnet_fpn . resnet101 ( pretrained = True ) Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value=''))) resnet_152_backbone = backbones . resnet_fpn . resnet152 ( pretrained = True ) Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value=''))) It supports backbones \"resnet18\", \"resnet34\", \"resnet50\",\"resnet101\", \"resnet152\", \"resnext101_32x8d\", \"mobilenet\", \"vgg11\", \"vgg13\", \"vgg16\", \"vgg19\", without fpn networks vgg11_backbone = backbones . vgg . vgg11 ( pretrained = True ) Downloading: \"https://download.pytorch.org/models/vgg11-bbd30ac9.pth\" to /root/.cache/torch/checkpoints/vgg11-bbd30ac9.pth HBox(children=(FloatProgress(value=0.0, max=531456000.0), HTML(value='')))","title":"Backbones"},{"location":"advanced_guide/#mantisshrimp-engines","text":"Now engine enters the picture. We provide out of box support to train with Fastai and PyTorch Lightning. You can use any one, at end of the day both remain PyTorch models.","title":"Mantisshrimp Engines"},{"location":"advanced_guide/#training-using-fastai","text":"We provide support for COCO Metrics too, very useful while training # metrics = [] # metrics += [COCOMetric(valid_rs, bbox=True, mask=False, keypoint=False)]","title":"Training Using Fastai"},{"location":"advanced_guide/#dataloader","text":"Another feature is that all mantis models have a dataloader method that returns a customized DataLoader for each model. train_dl = faster_rcnn . dataloaders . train_dataloader ( train_ds , batch_size = 4 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . dataloaders . valid_dataloader ( valid_ds , batch_size = 4 , num_workers = 4 , shuffle = False ) # This creates the default model with resnet50 fpn backbone # model = faster_rcnn.model(num_classes=2) # To create model with backbones model = faster_rcnn . model ( num_classes = 2 , backbone = resnet_152_backbone ) learner = faster_rcnn . fastai . learner ([ train_dl , valid_dl ], model ) learner . fine_tune ( 2 , lr = 1e-4 ) /opt/conda/lib/python3.7/site-packages/fastai2/callback/core.py:29: UserWarning: You are setting an attribute (loss) that also exists in the learner. Please be advised that you're not setting it in the learner but in the callback. Use `self.learn.loss` if you would like to change it in the learner. warn(f\"You are setting an attribute ({name}) that also exists in the learner. Please be advised that you're not setting it in the learner but in the callback. Use `self.learn.{name}` if you would like to change it in the learner.\") epoch train_loss valid_loss time 0 1.262132 1.240003 10:35 /opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated: nonzero(Tensor input, *, Tensor out) Consider using one of the following signatures instead: nonzero(Tensor input, *, bool as_tuple) epoch train_loss valid_loss time 0 0.682398 0.660053 18:55 1 0.576909 0.555283 18:53","title":"DataLoader"},{"location":"advanced_guide/#training-using-pytorch-lightning","text":"class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): opt = SGD ( self . parameters (), 2e-4 , momentum = 0.9 ) return opt model = faster_rcnn . model ( num_classes = 2 , backbone = resnet_152_backbone ) light_model = LightModel ( model = model ) #metrics=metrics) from pytorch_lightning import Trainer trainer = Trainer ( max_epochs = 2 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) GPU available: True, used: True TPU available: False, using: 0 TPU cores CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params ------------------------------------- 0 | model | FasterRCNN | 75 M HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout\u2026 /opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated: nonzero(Tensor input, *, Tensor out) Consider using one of the following signatures instead: nonzero(Tensor input, *, bool as_tuple) HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max\u2026 HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m\u2026 HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m\u2026 1","title":"Training using PyTorch Lightning"},{"location":"advanced_guide/#saving-the-model","text":"# Save the model as same as you would do for a Pytorch model # You can also use lightning features to even automate this. torch . save ( light_model . state_dict (), \"mantiss_faster_rcnn.pt\" )","title":"Saving the Model"},{"location":"advanced_guide/#making-predictions","text":"detection_threshold = 0.45 device = torch . device ( 'cuda' ) if torch . cuda . is_available () else torch . device ( 'cpu' ) def format_prediction_string ( boxes , scores ): pred_strings = [] for s , b in zip ( scores , boxes . astype ( int )): pred_strings . append ( f ' { s : .4f } { b [ 0 ] } { b [ 1 ] } { b [ 2 ] - b [ 0 ] } { b [ 3 ] - b [ 1 ] } ' ) return \" \" . join ( pred_strings ) # Just using the Fastai model for now. You can replace this with Light model as well. model . eval () model . to ( device ) detection_threshold = 1e-8 results = [] device = 'cuda' for images in os . listdir ( \"../input/global-wheat-detection/test/\" ): image_path = os . path . join ( \"../input/global-wheat-detection/test/\" , images ) image = cv2 . imread ( image_path ) image = np . transpose ( image , ( 2 , 0 , 1 )) image = image / 255. image = torch . tensor ( image , dtype = torch . float ) image = torch . unsqueeze ( image , 0 ) image = image . to ( device ) model # print(image.shape) with torch . no_grad (): outputs = model ( image ) # print(outputs) boxes = outputs [ 0 ][ 'boxes' ] . data . cpu () . numpy () scores = outputs [ 0 ][ 'scores' ] . data . cpu () . numpy () boxes = boxes [ scores >= detection_threshold ] . astype ( np . int32 ) scores = scores [ scores >= detection_threshold ] image_id = images [: - 3 ] result = { 'image_id' : image_id , 'PredictionString' : format_prediction_string ( boxes , scores ) } results . append ( result ) # break test_df = pd . DataFrame ( results , columns = [ 'image_id' , 'PredictionString' ]) test_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id PredictionString 0 796707dd7. 0.9835 892 326 108 95 0.9809 702 824 111 105 0.9749 499 780 102 102 0.9732 0 445 68 90 0.9714 349 0 83 61 0.9678 939 66 82 111 0.9501 134 545 138 110 0.9491 675 709 82 100 0.9306 611 34 100 83 0.9283 103 792 100 88 0.9190 44 89 170 106 0.9069 249 474 93 106 0.9055 674 27 126 129 0.8951 680 448 101 154 0.8831 374 621 99 140 0.8831 461 269 122 103 0.8787 220 835 122 82 0.8770 426 185 83 89 0.8714 246 322 82 161 0.8611 301 275 88 125 0.8575 663 559 122 151 0.8444 801 530 135 154 0.8337 235 0 113 47 0.8137 388 619 82 79 0.7730 214 675 91 82 0.6770 194 471 81 77 0.6689 592 490 79 77 0.6685 59 5... 1 cc3532ff6. 0.9866 369 0 97 106 0.9841 911 125 106 92 0.9761 606 423 94 106 0.9742 489 568 100 149 0.9719 258 631 107 179 0.9694 689 471 151 93 0.9693 470 395 124 162 0.9675 716 294 118 95 0.9632 552 826 96 100 0.9520 33 342 98 73 0.9487 306 286 92 87 0.9417 96 609 86 151 0.9375 401 206 87 88 0.9368 755 829 182 176 0.9332 0 418 164 99 0.9327 779 696 111 108 0.9302 72 816 138 162 0.9267 952 1 72 86 0.9106 564 299 104 112 0.8850 769 2 101 128 0.8816 14 657 104 71 0.7850 6 475 73 78 0.7729 26 644 173 87 0.7387 10 363 133 179 0.7025 583 889 103 112 0.6457 0 769 43 71 0.6198 291 839 101 66 0.5801 10 404 84... 2 51f1be19e. 0.9569 18 0 99 68 0.9546 274 464 155 134 0.9402 765 886 140 100 0.9393 636 793 111 76 0.9368 194 917 107 101 0.9321 688 914 98 84 0.9262 835 338 136 119 0.9229 600 95 153 166 0.9224 811 84 108 75 0.9153 815 757 91 86 0.9068 325 141 142 138 0.8867 902 693 105 82 0.8552 669 595 104 84 0.8491 240 117 88 119 0.8478 492 487 111 85 0.8331 860 271 104 204 0.8250 69 692 91 84 0.8121 563 588 113 128 0.7668 485 477 202 92 0.7495 0 379 49 106 0.6596 527 970 91 54 0.6227 534 462 178 89 0.5940 46 520 70 80 0.5751 659 428 83 72 0.5720 775 24 102 65 0.5616 74 684 82 165 0.5557 850 276 117 104 0.4984 120 ... 3 51b3e36ab. 0.9824 361 151 110 106 0.9809 471 577 88 85 0.9798 499 190 102 81 0.9786 0 901 86 121 0.9712 231 635 98 180 0.9694 420 925 88 99 0.9683 12 823 90 98 0.9578 440 323 150 97 0.9573 856 711 82 81 0.9540 377 424 108 82 0.9531 874 185 131 80 0.9519 462 4 87 164 0.9514 873 287 151 136 0.9464 329 466 91 152 0.9425 600 762 171 115 0.9412 113 848 143 90 0.9127 755 817 208 122 0.9046 534 29 271 129 0.8943 503 394 120 93 0.8894 11 435 82 213 0.8806 822 455 199 155 0.8713 609 374 174 95 0.8594 6 0 79 172 0.7965 7 580 87 188 0.7126 707 670 157 80 0.6970 1 412 103 365 0.5088 448 320 173 162 0.4831 553 24... 4 f5a1f0358. 0.9833 543 401 84 96 0.9782 683 197 122 101 0.9779 411 168 68 74 0.9767 63 457 136 119 0.9727 258 657 95 82 0.9725 659 104 88 92 0.9723 596 723 109 93 0.9701 224 317 124 99 0.9700 815 404 100 99 0.9684 1 831 65 131 0.9676 297 455 155 103 0.9672 881 628 97 168 0.9626 153 243 69 83 0.9616 149 758 163 123 0.9600 126 612 72 79 0.9447 224 558 80 92 0.9405 299 568 76 80 0.9387 2 5 64 66 0.9322 541 277 108 113 0.9281 692 564 85 135 0.9183 527 0 110 105 0.9134 405 677 84 123 0.9094 949 440 75 192 0.8918 87 818 67 75 0.8481 470 796 135 87 0.8111 459 314 85 156 0.6798 783 578 86 97 0.5494 459 575 56...","title":"Making Predictions"},{"location":"changing-the-colors/","text":"Changing the colors If you install the documentation on your local machine, you can pick the colors of your choice. The colors can be set from mkdocs.yml located in the docs/ folder Color scheme Our documenation supports two color schemes : a light mode, which is just called default , and a dark mode, which is called slate . The color scheme can be set from mkdocs.yml : theme : palette : scheme : default click on a tile to change the color scheme: default slate var buttons = document.querySelectorAll(\"button[data-md-color-scheme]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-scheme\") document.body.setAttribute(\"data-md-color-scheme\", attr) var name = document.querySelector(\"#__code_0 code span:nth-child(7)\") name.textContent = attr }) }) Primary color The primary color is used for the header, the sidebar, text links and several other components. In order to change the primary color, set the following value in mkdocs.yml to a valid color name: theme : palette : primary : indigo click on a tile to change the primary color: red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-primary\") document.body.setAttribute(\"data-md-color-primary\", attr) var name = document.querySelector(\"#__code_2 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accent color The accent color is used to denote elements that can be interacted with, e.g. hovered links, buttons and scrollbars. It can be changed in mkdocs.yml by chosing a valid color name: theme : palette : accent : indigo click on a tile to change the accent color: .md-typeset button[data-md-color-accent] > code { background-color: var(--md-code-bg-color); color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-accent\") document.body.setAttribute(\"data-md-color-accent\", attr) var name = document.querySelector(\"#__code_3 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accessibility \u2013 not all color combinations work well With 2 (color schemes) x 21 (primary colors) x 17 (accent color) = 714 combinations, it's impossible to ensure that all configurations provide a good user experience (e.g. yellow on light background ), so make sure that the color combination of your choosing provides enough contrast and tweak CSS variables where necessary.","title":"Changing the colors"},{"location":"changing-the-colors/#changing-the-colors","text":"If you install the documentation on your local machine, you can pick the colors of your choice. The colors can be set from mkdocs.yml located in the docs/ folder","title":"Changing the colors"},{"location":"changing-the-colors/#color-scheme","text":"Our documenation supports two color schemes : a light mode, which is just called default , and a dark mode, which is called slate . The color scheme can be set from mkdocs.yml : theme : palette : scheme : default click on a tile to change the color scheme: default slate var buttons = document.querySelectorAll(\"button[data-md-color-scheme]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-scheme\") document.body.setAttribute(\"data-md-color-scheme\", attr) var name = document.querySelector(\"#__code_0 code span:nth-child(7)\") name.textContent = attr }) })","title":"Color scheme"},{"location":"changing-the-colors/#primary-color","text":"The primary color is used for the header, the sidebar, text links and several other components. In order to change the primary color, set the following value in mkdocs.yml to a valid color name: theme : palette : primary : indigo click on a tile to change the primary color: red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-primary\") document.body.setAttribute(\"data-md-color-primary\", attr) var name = document.querySelector(\"#__code_2 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) })","title":"Primary color"},{"location":"changing-the-colors/#accent-color","text":"The accent color is used to denote elements that can be interacted with, e.g. hovered links, buttons and scrollbars. It can be changed in mkdocs.yml by chosing a valid color name: theme : palette : accent : indigo click on a tile to change the accent color: .md-typeset button[data-md-color-accent] > code { background-color: var(--md-code-bg-color); color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-accent\") document.body.setAttribute(\"data-md-color-accent\", attr) var name = document.querySelector(\"#__code_3 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accessibility \u2013 not all color combinations work well With 2 (color schemes) x 21 (primary colors) x 17 (accent color) = 714 combinations, it's impossible to ensure that all configurations provide a good user experience (e.g. yellow on light background ), so make sure that the color combination of your choosing provides enough contrast and tweak CSS variables where necessary.","title":"Accent color"},{"location":"contributing/","text":"Contribution Guide We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps Step 1: Forking and Installing MantisShrimp \u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream mantisshrimp repo. \u200b2. Download a copy of your remote username/mantisshrimp repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/mantisshrimp.git Install the requirments. You many use miniconda or conda as well. pip install -r requirements.txt Step 2: Stay in Sync with the original (upstream) repo Set the upstream to sync with this repo. This will keep you in sync with mantisshrimp easily. git remote add upstream https://github.com/lgvaz/mantisshrimp.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master Step 3: Creating a new branch git checkout -b feature-name git branch master * feature_name: Step 4: Make changes, and commit your file changes Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files git add path/to/file.md git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary. Step 5: Submitting a Pull Request A. Method 1: Using GitHub CLI Preliminary step (done only once): Install gh by following the instructions in docs . 1. Create a pull request using GitHub CLI # Fill up the PR title and the body gh pr create -B master -b \"enter body of PR here\" -t \"enter title\" 2. Confirm PR was created You can confirm that your PR has been created by running the following command, from the mantisshrimp folder: gh pr list You can also check the status of your PR by running: gh pr status More detailed documentation can be found https://cli.github.com/manual/gh_pr . 3. Updating a PR If you want to change your code after a PR has been created, you can do it by sending more commits to the same remote branch. For example: git commit -m \"updated the feature\" git push origin <enter-branch-name-same-as-before> It will automatically show up in the PR on the github page. If these are small changes they can be squashed together by the reviewer at the merge time and appear as a single commit in the repository. B. Method 2: Using Git 1. Create a pull request git Upload your local branch to your remote GitHub repo (github.com/username/mantisshrimp) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the mantisshrimp main repo and GitHub will prompt you to create a pull request. 2. Confirm PR was created: Ensure your pr is listed here Updating a PR: Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" git push origin <enter-branch-name-same-as-before> Reviewing Your PR Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream mantisshrimp repo. note MantisShrimp has CI checking. It will automatically check your code for build as well. Feature Requests and questions For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Contributing Guide"},{"location":"contributing/#contribution-guide","text":"We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps","title":"Contribution Guide"},{"location":"contributing/#step-1-forking-and-installing-mantisshrimp","text":"\u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream mantisshrimp repo. \u200b2. Download a copy of your remote username/mantisshrimp repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/mantisshrimp.git Install the requirments. You many use miniconda or conda as well. pip install -r requirements.txt","title":"Step 1: Forking and Installing MantisShrimp"},{"location":"contributing/#step-2-stay-in-sync-with-the-original-upstream-repo","text":"Set the upstream to sync with this repo. This will keep you in sync with mantisshrimp easily. git remote add upstream https://github.com/lgvaz/mantisshrimp.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master","title":"Step 2: Stay in Sync with the original (upstream) repo"},{"location":"contributing/#step-3-creating-a-new-branch","text":"git checkout -b feature-name git branch master * feature_name:","title":"Step 3: Creating a new branch"},{"location":"contributing/#step-4-make-changes-and-commit-your-file-changes","text":"Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files git add path/to/file.md git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary.","title":"Step 4: Make changes, and commit your file changes"},{"location":"contributing/#step-5-submitting-a-pull-request","text":"","title":"Step 5: Submitting a Pull Request"},{"location":"contributing/#a-method-1-using-github-cli","text":"Preliminary step (done only once): Install gh by following the instructions in docs .","title":"A. Method 1: Using GitHub CLI"},{"location":"contributing/#1-create-a-pull-request-using-github-cli","text":"# Fill up the PR title and the body gh pr create -B master -b \"enter body of PR here\" -t \"enter title\"","title":"1. Create a pull request using GitHub CLI"},{"location":"contributing/#2-confirm-pr-was-created","text":"You can confirm that your PR has been created by running the following command, from the mantisshrimp folder: gh pr list You can also check the status of your PR by running: gh pr status More detailed documentation can be found https://cli.github.com/manual/gh_pr .","title":"2. Confirm PR was created"},{"location":"contributing/#3-updating-a-pr","text":"If you want to change your code after a PR has been created, you can do it by sending more commits to the same remote branch. For example: git commit -m \"updated the feature\" git push origin <enter-branch-name-same-as-before> It will automatically show up in the PR on the github page. If these are small changes they can be squashed together by the reviewer at the merge time and appear as a single commit in the repository.","title":"3. Updating a PR"},{"location":"contributing/#b-method-2-using-git","text":"","title":"B. Method 2: Using Git"},{"location":"contributing/#1-create-a-pull-request-git","text":"Upload your local branch to your remote GitHub repo (github.com/username/mantisshrimp) git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the mantisshrimp main repo and GitHub will prompt you to create a pull request.","title":"1. Create a pull request git"},{"location":"contributing/#2-confirm-pr-was-created_1","text":"Ensure your pr is listed here Updating a PR: Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" git push origin <enter-branch-name-same-as-before>","title":"2. Confirm PR was created:"},{"location":"contributing/#reviewing-your-pr","text":"Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream mantisshrimp repo. note MantisShrimp has CI checking. It will automatically check your code for build as well.","title":"Reviewing Your PR"},{"location":"contributing/#feature-requests-and-questions","text":"For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Feature Requests and questions"},{"location":"custom_parser/","text":"Custom Parsers from mantisshrimp.imports import * from mantisshrimp import * import pandas as pd Understand the data format In this task we were given a .csv file with annotations, let's take a look at that. **Note:** Replace `source` with your own path for the dataset directory. source = Path ( \"/home/lgvaz/.data/wheat\" ) df = pd . read_csv ( source / \"train.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 0 b6ab77fd7 1024 1024 [834.0, 222.0, 56.0, 36.0] usask_1 1 b6ab77fd7 1024 1024 [226.0, 548.0, 130.0, 58.0] usask_1 2 b6ab77fd7 1024 1024 [377.0, 504.0, 74.0, 160.0] usask_1 3 b6ab77fd7 1024 1024 [834.0, 95.0, 109.0, 107.0] usask_1 4 b6ab77fd7 1024 1024 [26.0, 144.0, 124.0, 117.0] usask_1 At first glance, we can make the following assumptions: Multiple rows with the same object_id, width, height A different bbox for each row * source doesn't seem relevant right now Once we know what our data provides we can create our custom Parser . Create the Parser When creating a Parser we inherit from smaller building blocks that provides the functionallity we want: DefaultImageInfoParser : Will parse standard fields for image information, e.g. filepath , height , width FasterRCNNParser : Since we only need to predict bboxes we will use a FasterRCNN model, this will parse all the requirements for using such a model. We can also specify exactly what fields we would like to parse, in fact, the parsers we are currently using are just helper classes that groups a collection of individual parsers. We are going to see how to use individual parsers in a future tutorial. **Note:** If you are using an IDE there is a little bit of magic than can happen. Once you created defined your class you can right click on it and select the option _\"implement abstract methods\"_, this will automatically populate your class with all the methods you need to override. If you are using a notebook, or your IDE does not support that, check the documentation to know what methods you should override. **Important:** Be sure to return the correct type on all overriden methods! class WheatParser ( DefaultImageInfoParser , FasterRCNNParser ): def __init__ ( self , df , source ): self . df = df self . source = source self . imageid_map = IDMap () def __iter__ ( self ): yield from self . df . itertuples () def __len__ ( self ): return len ( self . df ) def imageid ( self , o ) -> int : return self . imageid_map [ o . image_id ] def filepath ( self , o ) -> Union [ str , Path ]: return self . source / f \" { o . image_id } .jpg\" def height ( self , o ) -> int : return o . height def width ( self , o ) -> int : return o . width def labels ( self , o ) -> List [ int ]: return [ 1 ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))] Defining the __init__ is completely up to you, normally we have to pass our data (the df in our case) and the folder where our images are contained ( source in our case). We then override __iter__ , telling our parser how to iterate over our data. In our case we call df.itertuples to iterate over all df rows. __len__ is not obligatory but will help visualizing the progress when parsing. And finally we override all the other methods, they all receive a single argument o , which is the object returned by __iter__ . Now we just need to decide how to split our data and Parser.parse ! data_splitter = RandomSplitter ([ . 8 , . 2 ]) parser = WheatParser ( df , source / \"train\" ) train_rs , valid_rs = parser . parse ( data_splitter ) HBox(children=(FloatProgress(value=0.0, max=147793.0), HTML(value=''))) Let's take a look at one record. show_record ( train_rs [ 0 ], label = False ) Conclusion And that's it! Now that you have your data in the standard library record format, you can use it to create a Dataset , visualize the image with the annotations and basically use all helper functions that Mantisshrimp provides!","title":"Custom Parser"},{"location":"custom_parser/#custom-parsers","text":"from mantisshrimp.imports import * from mantisshrimp import * import pandas as pd","title":"Custom Parsers"},{"location":"custom_parser/#understand-the-data-format","text":"In this task we were given a .csv file with annotations, let's take a look at that. **Note:** Replace `source` with your own path for the dataset directory. source = Path ( \"/home/lgvaz/.data/wheat\" ) df = pd . read_csv ( source / \"train.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 0 b6ab77fd7 1024 1024 [834.0, 222.0, 56.0, 36.0] usask_1 1 b6ab77fd7 1024 1024 [226.0, 548.0, 130.0, 58.0] usask_1 2 b6ab77fd7 1024 1024 [377.0, 504.0, 74.0, 160.0] usask_1 3 b6ab77fd7 1024 1024 [834.0, 95.0, 109.0, 107.0] usask_1 4 b6ab77fd7 1024 1024 [26.0, 144.0, 124.0, 117.0] usask_1 At first glance, we can make the following assumptions: Multiple rows with the same object_id, width, height A different bbox for each row * source doesn't seem relevant right now Once we know what our data provides we can create our custom Parser .","title":"Understand the data format"},{"location":"custom_parser/#create-the-parser","text":"When creating a Parser we inherit from smaller building blocks that provides the functionallity we want: DefaultImageInfoParser : Will parse standard fields for image information, e.g. filepath , height , width FasterRCNNParser : Since we only need to predict bboxes we will use a FasterRCNN model, this will parse all the requirements for using such a model. We can also specify exactly what fields we would like to parse, in fact, the parsers we are currently using are just helper classes that groups a collection of individual parsers. We are going to see how to use individual parsers in a future tutorial. **Note:** If you are using an IDE there is a little bit of magic than can happen. Once you created defined your class you can right click on it and select the option _\"implement abstract methods\"_, this will automatically populate your class with all the methods you need to override. If you are using a notebook, or your IDE does not support that, check the documentation to know what methods you should override. **Important:** Be sure to return the correct type on all overriden methods! class WheatParser ( DefaultImageInfoParser , FasterRCNNParser ): def __init__ ( self , df , source ): self . df = df self . source = source self . imageid_map = IDMap () def __iter__ ( self ): yield from self . df . itertuples () def __len__ ( self ): return len ( self . df ) def imageid ( self , o ) -> int : return self . imageid_map [ o . image_id ] def filepath ( self , o ) -> Union [ str , Path ]: return self . source / f \" { o . image_id } .jpg\" def height ( self , o ) -> int : return o . height def width ( self , o ) -> int : return o . width def labels ( self , o ) -> List [ int ]: return [ 1 ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))] Defining the __init__ is completely up to you, normally we have to pass our data (the df in our case) and the folder where our images are contained ( source in our case). We then override __iter__ , telling our parser how to iterate over our data. In our case we call df.itertuples to iterate over all df rows. __len__ is not obligatory but will help visualizing the progress when parsing. And finally we override all the other methods, they all receive a single argument o , which is the object returned by __iter__ . Now we just need to decide how to split our data and Parser.parse ! data_splitter = RandomSplitter ([ . 8 , . 2 ]) parser = WheatParser ( df , source / \"train\" ) train_rs , valid_rs = parser . parse ( data_splitter ) HBox(children=(FloatProgress(value=0.0, max=147793.0), HTML(value=''))) Let's take a look at one record. show_record ( train_rs [ 0 ], label = False )","title":"Create the Parser"},{"location":"custom_parser/#conclusion","text":"And that's it! Now that you have your data in the standard library record format, you can use it to create a Dataset , visualize the image with the annotations and basically use all helper functions that Mantisshrimp provides!","title":"Conclusion"},{"location":"deployment/","text":"Deployment using Streamlit We provide a nice demo using streamlit . If streamlit is not already install, run the following command, from the terminal to install it: pip install streamlit Simply run the following in your terminal. It should start a demo in your browser. It will show you one of our trained models as a pet detector!! streamlit run https://raw.githubusercontent.com/oke-aditya/mantisshrimp_streamlit/master/app.py You can find the source code of this demo here . You can also use it as a template when creating your own streamlit apps with mantisshrimp.","title":"Deployment"},{"location":"deployment/#deployment-using-streamlit","text":"We provide a nice demo using streamlit . If streamlit is not already install, run the following command, from the terminal to install it: pip install streamlit Simply run the following in your terminal. It should start a demo in your browser. It will show you one of our trained models as a pet detector!! streamlit run https://raw.githubusercontent.com/oke-aditya/mantisshrimp_streamlit/master/app.py You can find the source code of this demo here . You can also use it as a template when creating your own streamlit apps with mantisshrimp.","title":"Deployment using Streamlit"},{"location":"docker/","text":"Mantisshrimp Docker Quick Start: Use Mantisshrimp Docker Container To jumpstart using mantisshrimp package without manually installing it and its dependencies, use our docker container! Please, follow the 3 steps: Install Docker by following the instructions shown in Docker website (Only if Docker is not already installed) In the terminal, download Mantisshrimp docker image to your machine: docker pull mantisshrimp In the terminal, run the mantisshrimp docker container: docker run -it mantisshrimp","title":"Docker"},{"location":"docker/#mantisshrimp-docker","text":"","title":"Mantisshrimp Docker"},{"location":"docker/#quick-start-use-mantisshrimp-docker-container","text":"To jumpstart using mantisshrimp package without manually installing it and its dependencies, use our docker container! Please, follow the 3 steps: Install Docker by following the instructions shown in Docker website (Only if Docker is not already installed) In the terminal, download Mantisshrimp docker image to your machine: docker pull mantisshrimp In the terminal, run the mantisshrimp docker container: docker run -it mantisshrimp","title":"Quick Start: Use Mantisshrimp Docker Container"},{"location":"getting_started/","text":"Getting started with Mantisshrimp Why Mantishrimp? It is an Agnostic Object-Detection Library Connects to different libraries/framework such as fastai, Pytorch Lightning, and Pytorch Features a Unified Data API such: common Parsers (COCO, VOC, etc.) Integrates community maintaned parsers for custom datasets shared on parsers hub Provides flexible model implementations using different backbones Helps both researchers and DL engineers in reproducing, replicating published models Facilitates applying both existing and new models to standard datasets as well as to Scustom datasets 1- Introduction This tutorial walk you throug the different steps of training the PETS dataset. the Mantisshrimp Framework is an agnostic framework . As an illustration, we will train our model using both the fastai2 , and pytorch-lightning library. 2- Using Google Colab First, enable the GPU runtime: Runtime -> Change runtime type -> Hardware accelerator dropdown -> GPU 3- Packages Installations # Install Mantisshrimp package ! pip install git + git : // github . com / lgvaz / mantisshrimp . git # Install cocoapi and albumentations packages ! pip install - U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' ! pip install albumentations -- upgrade # Install fastai and/or Pytorch-Lightning ! pip install fastai2 ! pip install pytorch - lightning 4- Imports from mantisshrimp.imports import * from mantisshrimp import * import albumentations as A 5- Datasets : PETS example Mantisshrimp provides very handy methods such as loading a dataset, parsing annotations, and more. In the example below, loading the PETS dataset is one line code. So is the pasrser. path = datasets . pets . load () path 5.1- Parser The Parser is one of the most important concepts of mantisshrimp, it's what allow us to work with any annotation format. The basic idea of a parser is to convert any custom format to something the library can understand. We provide a wide variety of parsers via the datasets modules (each dataset will come with it's own parser), but you might still need to create a custom parser for your custom dataset. Fear not! Creating parsers is very easy, after you're finished with this tutorial, check this documentation section to understand how to do so. Mantisshrimp already provide a parser for the Pets Dataset parser = datasets . pets . parser ( path ) dir ( datasets . pets ) # For convenience CLASSES = datasets . pets . CLASSES 5.2- Split the dataset Next step is to define the train/valid splits for the data, let's use random splits for this one: data_splitter = RandomSplitter ([ . 8 , . 2 ]) 5.3- Parsing data Calling the parse() by passing the data splitter returns 2 records lists: one for the training and another for the validation datasets. train_records , valid_records = parser . parse ( data_splitter ) **Important** A record is a dictionary that contains all parsed fields defined by the parser used. No matter what format the annotations used, a record has a common structure that can be connected to different DL frameworks (fastai, Pytorch-Lightning, etc.) 5.3- Visualization Showing one single record (image + box + label) show_record ( train_records [ 0 ]) Showing label instead of the class ID show_record ( train_records [ 0 ], classes = CLASSES ) Showing a batch of images with their corresponding boxes and labels records = train_records [: 6 ] show_records ( records , ncols = 3 , classes = CLASSES ) 5.4- Plot Pets dataset height and width histogram all_records = train_records + valid_records _ = datasets . pets . plot_size_histogram ( all_records ) 6- Transforms Transforms is an essential stage of any training pipeline, you can find a multitude of different transforms libraries online: albumentations , solt , torchvision , only to cite a few. With mantisshrimp you can use any transforms library, you just need to inherit and override all abstract methods of the Transform class. To ease the user experience, we support for the widely used albumentations library, out-of-the-box. We plan to add more, in the future. 6.1- Train and Validation Dataset Transforms from mantisshrimp.datasets.pets.transforms import * # Use predefined transforms or create your own train_tfms = train_albumentations_tfms_pets () valid_tfms = valid_albumentations_tfms_pets () 7- Dataset Not to be confused with our previous datasets module, Dataset is a class that combines the records and transforms. For creating a Dataset we just need need to pass the parsed records from the previous step and optionally the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) What a `Dataset` class does: * Prepares the record: For example, in the record we just have a filename that points to the image, it's at this stage that we open the image. * Apply the pipeline of transforms to record processed in the previous step Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. **Important:** Because we normalized our images with `imagenet_stats`, when displaying transformed images, we need to denormalize them. The `show_sample` function receives an optional argument called `denormalize_fn` that we can be passed: In our case, we pass `denormalize_imagenet`. 7.1- Displaying the same image with different transforms samples = [ train_ds [ 4 ] for _ in range ( 6 )] show_samples ( samples , ncols = 3 , classes = CLASSES , denormalize_fn = denormalize_imagenet ) In this tutorial, we need to only predict bounding boxes, therefore we will use FasterRCNN . The only required argument we need to pass to the model is the number of classes of our dataset (which is simply the length of datasets.pets.CATEGORIES ) + 1 for the background. 8- Model model = MantisFasterRCNN ( num_classes = len ( CLASSES )) # Use `cuda` (GPU) model . to ( 'cuda' ) model . device device(type='cuda', index=0) 9- DataLoader Each model has its own dataloader (a pytorch DataLoader ) that ccould be customized: the dataloaders for the RCNN models have a custom collate function. train_dl = model . dataloader ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model . dataloader ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) 10- Training Mantisshrimp is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. 10.1- Training using fastai For getting access to the helper functions to train with fastai, just import as follows: # import fastai engine provided by the mantisshrimp modules from mantisshrimp.engines.fastai import * 10.1.1- Creating a Learner object Creating a fastai compatible Learner using the fastai familiar interface learn = rcnn_learner ( dls = [ train_dl , valid_dl ], model = model ) 10.1.2- Training the RCNN model using fastai fine_tune() method learn . fine_tune ( 10 , lr = 1e-4 ) epoch train_loss valid_loss time 0 0.283276 0.262968 05:01 epoch train_loss valid_loss time 0 0.209676 0.199116 08:39 1 0.187668 0.210870 08:38 2 0.147996 0.147964 08:37 3 0.123722 0.123347 08:36 4 0.107496 0.109913 08:35 5 0.093738 0.094762 08:36 6 0.077727 0.085455 08:35 7 0.057831 0.066012 08:33 8 0.048677 0.063397 08:33 9 0.043101 0.060794 08:33 10.2- Training using Pytorch-Lightning # import lightning engine provided by the mantisshrimp modules from mantisshrimp.engines.lightning import * 10.2.1- Creating a Pytorch-Lightning (PL) model class It inherits from RCNNLightningAdapter and implements the method PL configure_optimizers . class LightModel ( RCNNLightningAdapter ): def configure_optimizers ( self ): opt = SGD ( self . parameters (), 2e-4 , momentum = 0.9 ) return opt **Note:** If you are used with lightning, you may be wondering what happened with `training_step`, `validation_step` and methods that we ussually have to override while using PL. Under the hood `RCNNLightningAdapter` implements those methods with the additional bennefit of supporting `Metric`s. If you need more custom functionality, feel free to re-implement those methods. # Creating a PL model object light_model = LightModel ( model ) 10.2.2- Training the RCNN model using PL Trainer.fit() method trainer = Trainer ( max_epochs = 3 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) 11- Inference 11.1- Load a model Training the model with fastai using fine_tune twice and I got led tp the following results: train_loss: 0.06772 valid_loss: 0.074435 11.2- Using our Trained Weights If you don't want to train the model, you can use our rained weights that we publicly made vailable: You can download them with torch.hub : weights_url = \"https://mantisshrimp-models.s3.us-east-2.amazonaws.com/weights-384px-adam2%2B%2B.pth.zip\" state_dict = torch . hub . load_state_dict_from_url ( weights_url , map_location = torch . device ( \"cuda\" )) **Note:** Typically inference is done on the cpu, this is why we specify the paramater `map_location` to `cpu` when loading the state dict. Let's recreate the model and load the downloaded weights: model = MantisFasterRCNN ( num_classes = len ( CLASSES )) model . load_state_dict ( state_dict ) model . to ( 'cuda' ) model . device <All keys matched successfully> device(type='cuda', index=0) The first step for prediction is to have some images, let's grab some random ones from the validation dataset: 11.3- Predict all images at once samples = random . choices ( valid_ds , k = 6 ) images = [ sample [ \"img\" ] for sample in samples ] Every model has a predict method, the first argument should always be the a list of images, the other parameters might vary a bit, and are specific to the model you are using. For FasterRCNN we have detection_threshold , which specifies how confident the model should be to output a bounding box. preds = model . predict ( images , detection_threshold =. 8 ) Diplay a set of images with their corresponding predictions (boxes + labesl) show_preds ( images , preds , ncols = 3 , classes = CLASSES , denormalize_fn = denormalize_imagenet ) 11.4- Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. Had we have a test dataset, we woul have make our predicition using the batch technique mentionned here above. As illustrative example, we will predict all images belonging to the validation dataset using the following approach: dl = DataLoader ( valid_ds , batch_size = 8 , collate_fn = lambda o : o ) **Important:** The default `DataLoader` tries to collate (combine the items) of the batch in a very specific way, specifying `collate_fn=lambda o: o` is saying \"whatever you receive as a batch, return it without modifications\" because the validation dataset is already collated (processed). Using a simple iterator over the dataloader, we can make predictions for each batch: all_preds = [] for batch in dl : images = [ sample [ \"img\" ] for sample in batch ] preds = model . predict ( images , detection_threshold =. 8 ) all_preds . append ( preds ) all_preds [ 0 ] Happy Learning! If you need any assistance, feel free to reach out at us here","title":"Getting Started"},{"location":"getting_started/#getting-started-with-mantisshrimp","text":"","title":"Getting started with Mantisshrimp"},{"location":"getting_started/#why-mantishrimp","text":"It is an Agnostic Object-Detection Library Connects to different libraries/framework such as fastai, Pytorch Lightning, and Pytorch Features a Unified Data API such: common Parsers (COCO, VOC, etc.) Integrates community maintaned parsers for custom datasets shared on parsers hub Provides flexible model implementations using different backbones Helps both researchers and DL engineers in reproducing, replicating published models Facilitates applying both existing and new models to standard datasets as well as to Scustom datasets","title":"Why Mantishrimp?"},{"location":"getting_started/#1-introduction","text":"This tutorial walk you throug the different steps of training the PETS dataset. the Mantisshrimp Framework is an agnostic framework . As an illustration, we will train our model using both the fastai2 , and pytorch-lightning library.","title":"1- Introduction"},{"location":"getting_started/#2-using-google-colab","text":"First, enable the GPU runtime: Runtime -> Change runtime type -> Hardware accelerator dropdown -> GPU","title":"2- Using Google Colab"},{"location":"getting_started/#3-packages-installations","text":"# Install Mantisshrimp package ! pip install git + git : // github . com / lgvaz / mantisshrimp . git # Install cocoapi and albumentations packages ! pip install - U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' ! pip install albumentations -- upgrade # Install fastai and/or Pytorch-Lightning ! pip install fastai2 ! pip install pytorch - lightning","title":"3- Packages Installations"},{"location":"getting_started/#4-imports","text":"from mantisshrimp.imports import * from mantisshrimp import * import albumentations as A","title":"4- Imports"},{"location":"getting_started/#5-datasets-pets-example","text":"Mantisshrimp provides very handy methods such as loading a dataset, parsing annotations, and more. In the example below, loading the PETS dataset is one line code. So is the pasrser. path = datasets . pets . load () path","title":"5- Datasets : PETS example"},{"location":"getting_started/#51-parser","text":"The Parser is one of the most important concepts of mantisshrimp, it's what allow us to work with any annotation format. The basic idea of a parser is to convert any custom format to something the library can understand. We provide a wide variety of parsers via the datasets modules (each dataset will come with it's own parser), but you might still need to create a custom parser for your custom dataset. Fear not! Creating parsers is very easy, after you're finished with this tutorial, check this documentation section to understand how to do so. Mantisshrimp already provide a parser for the Pets Dataset parser = datasets . pets . parser ( path ) dir ( datasets . pets ) # For convenience CLASSES = datasets . pets . CLASSES","title":"5.1- Parser"},{"location":"getting_started/#52-split-the-dataset","text":"Next step is to define the train/valid splits for the data, let's use random splits for this one: data_splitter = RandomSplitter ([ . 8 , . 2 ])","title":"5.2- Split the dataset"},{"location":"getting_started/#53-parsing-data","text":"Calling the parse() by passing the data splitter returns 2 records lists: one for the training and another for the validation datasets. train_records , valid_records = parser . parse ( data_splitter ) **Important** A record is a dictionary that contains all parsed fields defined by the parser used. No matter what format the annotations used, a record has a common structure that can be connected to different DL frameworks (fastai, Pytorch-Lightning, etc.)","title":"5.3- Parsing data"},{"location":"getting_started/#53-visualization","text":"Showing one single record (image + box + label) show_record ( train_records [ 0 ]) Showing label instead of the class ID show_record ( train_records [ 0 ], classes = CLASSES ) Showing a batch of images with their corresponding boxes and labels records = train_records [: 6 ] show_records ( records , ncols = 3 , classes = CLASSES )","title":"5.3- Visualization"},{"location":"getting_started/#54-plot-pets-dataset-height-and-width-histogram","text":"all_records = train_records + valid_records _ = datasets . pets . plot_size_histogram ( all_records )","title":"5.4- Plot Pets dataset height and width histogram"},{"location":"getting_started/#6-transforms","text":"Transforms is an essential stage of any training pipeline, you can find a multitude of different transforms libraries online: albumentations , solt , torchvision , only to cite a few. With mantisshrimp you can use any transforms library, you just need to inherit and override all abstract methods of the Transform class. To ease the user experience, we support for the widely used albumentations library, out-of-the-box. We plan to add more, in the future.","title":"6- Transforms"},{"location":"getting_started/#61-train-and-validation-dataset-transforms","text":"from mantisshrimp.datasets.pets.transforms import * # Use predefined transforms or create your own train_tfms = train_albumentations_tfms_pets () valid_tfms = valid_albumentations_tfms_pets ()","title":"6.1- Train and Validation Dataset Transforms"},{"location":"getting_started/#7-dataset","text":"Not to be confused with our previous datasets module, Dataset is a class that combines the records and transforms. For creating a Dataset we just need need to pass the parsed records from the previous step and optionally the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) What a `Dataset` class does: * Prepares the record: For example, in the record we just have a filename that points to the image, it's at this stage that we open the image. * Apply the pipeline of transforms to record processed in the previous step Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. **Important:** Because we normalized our images with `imagenet_stats`, when displaying transformed images, we need to denormalize them. The `show_sample` function receives an optional argument called `denormalize_fn` that we can be passed: In our case, we pass `denormalize_imagenet`.","title":"7- Dataset"},{"location":"getting_started/#71-displaying-the-same-image-with-different-transforms","text":"samples = [ train_ds [ 4 ] for _ in range ( 6 )] show_samples ( samples , ncols = 3 , classes = CLASSES , denormalize_fn = denormalize_imagenet ) In this tutorial, we need to only predict bounding boxes, therefore we will use FasterRCNN . The only required argument we need to pass to the model is the number of classes of our dataset (which is simply the length of datasets.pets.CATEGORIES ) + 1 for the background.","title":"7.1- Displaying the same image with different transforms"},{"location":"getting_started/#8-model","text":"model = MantisFasterRCNN ( num_classes = len ( CLASSES )) # Use `cuda` (GPU) model . to ( 'cuda' ) model . device device(type='cuda', index=0)","title":"8- Model"},{"location":"getting_started/#9-dataloader","text":"Each model has its own dataloader (a pytorch DataLoader ) that ccould be customized: the dataloaders for the RCNN models have a custom collate function. train_dl = model . dataloader ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model . dataloader ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"9- DataLoader"},{"location":"getting_started/#10-training","text":"Mantisshrimp is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"10- Training"},{"location":"getting_started/#101-training-using-fastai","text":"For getting access to the helper functions to train with fastai, just import as follows: # import fastai engine provided by the mantisshrimp modules from mantisshrimp.engines.fastai import *","title":"10.1- Training using fastai"},{"location":"getting_started/#1011-creating-a-learner-object","text":"Creating a fastai compatible Learner using the fastai familiar interface learn = rcnn_learner ( dls = [ train_dl , valid_dl ], model = model )","title":"10.1.1- Creating a Learner object"},{"location":"getting_started/#1012-training-the-rcnn-model-using-fastai-fine_tune-method","text":"learn . fine_tune ( 10 , lr = 1e-4 ) epoch train_loss valid_loss time 0 0.283276 0.262968 05:01 epoch train_loss valid_loss time 0 0.209676 0.199116 08:39 1 0.187668 0.210870 08:38 2 0.147996 0.147964 08:37 3 0.123722 0.123347 08:36 4 0.107496 0.109913 08:35 5 0.093738 0.094762 08:36 6 0.077727 0.085455 08:35 7 0.057831 0.066012 08:33 8 0.048677 0.063397 08:33 9 0.043101 0.060794 08:33","title":"10.1.2- Training the RCNN model using fastai fine_tune() method"},{"location":"getting_started/#102-training-using-pytorch-lightning","text":"# import lightning engine provided by the mantisshrimp modules from mantisshrimp.engines.lightning import *","title":"10.2- Training using Pytorch-Lightning"},{"location":"getting_started/#1021-creating-a-pytorch-lightning-pl-model-class","text":"It inherits from RCNNLightningAdapter and implements the method PL configure_optimizers . class LightModel ( RCNNLightningAdapter ): def configure_optimizers ( self ): opt = SGD ( self . parameters (), 2e-4 , momentum = 0.9 ) return opt **Note:** If you are used with lightning, you may be wondering what happened with `training_step`, `validation_step` and methods that we ussually have to override while using PL. Under the hood `RCNNLightningAdapter` implements those methods with the additional bennefit of supporting `Metric`s. If you need more custom functionality, feel free to re-implement those methods. # Creating a PL model object light_model = LightModel ( model )","title":"10.2.1- Creating a Pytorch-Lightning (PL) model class"},{"location":"getting_started/#1022-training-the-rcnn-model-using-pl-trainerfit-method","text":"trainer = Trainer ( max_epochs = 3 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"10.2.2- Training the RCNN model using PL Trainer.fit() method"},{"location":"getting_started/#11-inference","text":"","title":"11- Inference"},{"location":"getting_started/#111-load-a-model","text":"Training the model with fastai using fine_tune twice and I got led tp the following results: train_loss: 0.06772 valid_loss: 0.074435","title":"11.1- Load a model"},{"location":"getting_started/#112-using-our-trained-weights","text":"If you don't want to train the model, you can use our rained weights that we publicly made vailable: You can download them with torch.hub : weights_url = \"https://mantisshrimp-models.s3.us-east-2.amazonaws.com/weights-384px-adam2%2B%2B.pth.zip\" state_dict = torch . hub . load_state_dict_from_url ( weights_url , map_location = torch . device ( \"cuda\" )) **Note:** Typically inference is done on the cpu, this is why we specify the paramater `map_location` to `cpu` when loading the state dict. Let's recreate the model and load the downloaded weights: model = MantisFasterRCNN ( num_classes = len ( CLASSES )) model . load_state_dict ( state_dict ) model . to ( 'cuda' ) model . device <All keys matched successfully> device(type='cuda', index=0) The first step for prediction is to have some images, let's grab some random ones from the validation dataset:","title":"11.2- Using our Trained Weights"},{"location":"getting_started/#113-predict-all-images-at-once","text":"samples = random . choices ( valid_ds , k = 6 ) images = [ sample [ \"img\" ] for sample in samples ] Every model has a predict method, the first argument should always be the a list of images, the other parameters might vary a bit, and are specific to the model you are using. For FasterRCNN we have detection_threshold , which specifies how confident the model should be to output a bounding box. preds = model . predict ( images , detection_threshold =. 8 ) Diplay a set of images with their corresponding predictions (boxes + labesl) show_preds ( images , preds , ncols = 3 , classes = CLASSES , denormalize_fn = denormalize_imagenet )","title":"11.3- Predict all images at once"},{"location":"getting_started/#114-predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. Had we have a test dataset, we woul have make our predicition using the batch technique mentionned here above. As illustrative example, we will predict all images belonging to the validation dataset using the following approach: dl = DataLoader ( valid_ds , batch_size = 8 , collate_fn = lambda o : o ) **Important:** The default `DataLoader` tries to collate (combine the items) of the batch in a very specific way, specifying `collate_fn=lambda o: o` is saying \"whatever you receive as a batch, return it without modifications\" because the validation dataset is already collated (processed). Using a simple iterator over the dataloader, we can make predictions for each batch: all_preds = [] for batch in dl : images = [ sample [ \"img\" ] for sample in batch ] preds = model . predict ( images , detection_threshold =. 8 ) all_preds . append ( preds ) all_preds [ 0 ]","title":"11.4- Predicting a batch of images"},{"location":"getting_started/#happy-learning","text":"If you need any assistance, feel free to reach out at us here","title":"Happy Learning!"},{"location":"install/","text":"A- Local Installation using pypi There are 3 ways to install mantisshrimp and its dependencies using pip install . Note : You can check out the following blog post: 3 ways to pip install a package for more a detailed explantion on how to choose the most convenient option for you. Option 1: Installing from pypi repository [Coming Soon!] All Packages To install mantisshrimp package and both Fastai and Pytorch-Lightning libraries, run the following command: pip install mantisshrimp [ all ] Mantisshrimp + Fastai To install mantisshrimp package and only the Fastai library, run the following command: pip install mantisshrimp [ fastai ] Mantisshrimp + Pytorch-Lightning To install mantisshrimp package and only the Pytorch-Lightning library, run the following command: pip install mantisshrimp [ pytorch_lightning ] Option 2: Installing a non-editable package from GitHub [Already Available] To install the mantisshrimp package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the mantisshrimp latest version (from the master branch) pip install git+git://github.com/lgvaz/mantisshrimp.git [ all ] Option 3: Installing an editable package from GitHub [Already Available] Note: This method is used by developers who are usually either: actively contributing to mantisshrimp project by adding new features or fixing bugs, or creating their own modules, and making sure that their source code stay in sync with the mantisshrimp latest version. All we have to do is to follow these 3 simple steps by running the following commands: git clone --depth = 1 https://github.com/lgvaz/mantisshrimp.git cd mantisshrimp pip install . [ all ] B- Local Installation using conda Use the following command in order to create a conda environment called mantis (the name is set in the environment.yml file) conda env create -f environment.yml Activating mantis conda environment To activate the newly created mantis virtual environment, run the following command: conda activate mantis Note: Once you activate the conda environment, follow the steps described, here above, in order to pip install the mantisshrimp package and its dependencies: A- Local Installation using pypi C- Common step: cocoapi Installation: for both pypi and conda installation C.1- Installing cocoapi in Linux: pip install \"git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\" C.2- Installing cocoapi in Windows: pycoco cannot be installed using the command above (see issue-185 in the cocoapi repository). We are using this workaround: pip install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\" D- Updating mantis conda environment To update mantis conda environment, all you need to do is update the content of your environment.yml file accordingly and then run the following command: conda env update -f environment.yml --prune","title":"Installation"},{"location":"install/#a-local-installation-using-pypi","text":"There are 3 ways to install mantisshrimp and its dependencies using pip install . Note : You can check out the following blog post: 3 ways to pip install a package for more a detailed explantion on how to choose the most convenient option for you.","title":"A- Local Installation using pypi"},{"location":"install/#option-1-installing-from-pypi-repository-coming-soon","text":"","title":"Option 1: Installing from pypi repository [Coming Soon!]"},{"location":"install/#all-packages","text":"To install mantisshrimp package and both Fastai and Pytorch-Lightning libraries, run the following command: pip install mantisshrimp [ all ]","title":"All Packages"},{"location":"install/#mantisshrimp-fastai","text":"To install mantisshrimp package and only the Fastai library, run the following command: pip install mantisshrimp [ fastai ]","title":"Mantisshrimp + Fastai"},{"location":"install/#mantisshrimp-pytorch-lightning","text":"To install mantisshrimp package and only the Pytorch-Lightning library, run the following command: pip install mantisshrimp [ pytorch_lightning ]","title":"Mantisshrimp + Pytorch-Lightning"},{"location":"install/#option-2-installing-a-non-editable-package-from-github-already-available","text":"To install the mantisshrimp package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the mantisshrimp latest version (from the master branch) pip install git+git://github.com/lgvaz/mantisshrimp.git [ all ]","title":"Option 2: Installing a non-editable package from GitHub [Already Available]"},{"location":"install/#option-3-installing-an-editable-package-from-github-already-available","text":"Note: This method is used by developers who are usually either: actively contributing to mantisshrimp project by adding new features or fixing bugs, or creating their own modules, and making sure that their source code stay in sync with the mantisshrimp latest version. All we have to do is to follow these 3 simple steps by running the following commands: git clone --depth = 1 https://github.com/lgvaz/mantisshrimp.git cd mantisshrimp pip install . [ all ]","title":"Option 3: Installing an editable package from GitHub [Already Available]"},{"location":"install/#b-local-installation-using-conda","text":"Use the following command in order to create a conda environment called mantis (the name is set in the environment.yml file) conda env create -f environment.yml","title":"B- Local Installation using conda"},{"location":"install/#activating-mantis-conda-environment","text":"To activate the newly created mantis virtual environment, run the following command: conda activate mantis Note: Once you activate the conda environment, follow the steps described, here above, in order to pip install the mantisshrimp package and its dependencies: A- Local Installation using pypi","title":"Activating mantis conda environment"},{"location":"install/#c-common-step-cocoapi-installation-for-both-pypi-and-conda-installation","text":"","title":"C- Common step: cocoapi Installation: for both pypi and conda installation"},{"location":"install/#c1-installing-cocoapi-in-linux","text":"pip install \"git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\"","title":"C.1- Installing cocoapi in Linux:"},{"location":"install/#c2-installing-cocoapi-in-windows","text":"pycoco cannot be installed using the command above (see issue-185 in the cocoapi repository). We are using this workaround: pip install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\"","title":"C.2- Installing cocoapi in Windows:"},{"location":"install/#d-updating-mantis-conda-environment","text":"To update mantis conda environment, all you need to do is update the content of your environment.yml file accordingly and then run the following command: conda env update -f environment.yml --prune","title":"D- Updating mantis conda environment"},{"location":"readme_mkdocs/","text":"Mantisshrimp Documentation The source for Mantisshrimp documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs . Building the documentation Install dependencies: pip install -r docs/requirements.txt pip install -e . to make sure that Python will import your modified version of Mantisshrimp. From the root directory, cd into the docs/ folder and run: python autogen.py # Generate md files, and copy assets into the docs_dir/ folder specified in mkdocs.yml file mkdocs serve # Starts a local webserver: localhost:8000 To build the site/ , from the root directory, cd into the docs/ folder and run: mkdocs build # Builds a static site in the site/ directory To locally test the site/ , from the docs/ directory, cd into the site/ folder and run: python -m http.server","title":"Generating Docs"},{"location":"readme_mkdocs/#mantisshrimp-documentation","text":"The source for Mantisshrimp documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs .","title":"Mantisshrimp Documentation"},{"location":"readme_mkdocs/#building-the-documentation","text":"Install dependencies: pip install -r docs/requirements.txt pip install -e . to make sure that Python will import your modified version of Mantisshrimp. From the root directory, cd into the docs/ folder and run: python autogen.py # Generate md files, and copy assets into the docs_dir/ folder specified in mkdocs.yml file mkdocs serve # Starts a local webserver: localhost:8000 To build the site/ , from the root directory, cd into the docs/ folder and run: mkdocs build # Builds a static site in the site/ directory To locally test the site/ , from the docs/ directory, cd into the site/ folder and run: python -m http.server","title":"Building the documentation"},{"location":"examples/inference/","text":"import albumentations as A # Maps from IDs to class names. `print(class_map)` for all available classes class_map = datasets . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained on `Tutorials->Getting Started` WEIGHTS_URL = \"https://mantisshrimp-models.s3.us-east-2.amazonaws.com/pets.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = AlbuTransform ([ A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dataloader ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl () samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"Inference"},{"location":"examples/training_using_fastai/","text":"Example showing how to train the PETS dataset using Mantisshrimp and fastai2 library from mantisshrimp.imports import * from mantisshrimp import * import albumentations as A # import fastai engine provided by the mantisshrimp modules from mantisshrimp.engines.fastai import * # Load the PETS dataset path = datasets . pets . load () # split dataset lists data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) # PETS parser: provided out-of-the-box parser = datasets . pets . parser ( path ) train_records , valid_records = parser . parse ( data_splitter ) # For convenience CLASSES = datasets . pets . CLASSES # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , classes = CLASSES ) # Create both training and validation datasets - using Albumentations transforms out of the box train_ds = Dataset ( train_records , train_albumentations_tfms_pets ()) valid_ds = Dataset ( valid_records , valid_albumentations_tfms_pets ()) # Create both training and validation dataloaders train_dl = model . dataloader ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = model . dataloader ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( CLASSES )) # Training the model using fastai2 learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , lr = 1e-4 )","title":"Training using Fastai"}]}